{"cells":[{"cell_type":"markdown","metadata":{"id":"NjkyYaRRUKYO"},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyFLRsmxUCSs"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqqrXhFfUJq9"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import MinMaxScaler\n","!pip install imperio\n","import imperio\n","import tensorflow as tf\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92i0-ZvkURpj"},"outputs":[],"source":["!pip install fuzzywuzzy\n","from fuzzywuzzy import fuzz, process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNp5XwBSUS9i"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"b5G1Nkx-UW3h"},"source":["# Import data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k767y47gUZoM"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Organizatii/Sigmoid/Padawan Learning/Exam/data2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u61S1MNnUbKd"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"qesCM_vgUd-o"},"source":["# Analyzing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDe0alhbUf_U"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"Y-mzYzUyUljR"},"source":["There could be observed that Some coulmns (Population, GDP, Hepatitis B) has significant number of NaN data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9-AjwPmUiQy"},"outputs":[],"source":["plt.figure(figsize=(14,12))\n","sns.heatmap(df.corr())\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"r3IipO70VpeM"},"source":["\"thinness 5-9\" is repeating in \"thinness 1-19\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55cXkMlAUoca"},"outputs":[],"source":["status_groupby = df.groupby('Status', as_index=False)\n","status_groupby['Life expectancy '].describe()"]},{"cell_type":"markdown","metadata":{"id":"aBOVhbiAWPSO"},"source":["From the above table there could be possible to extract following information:\n","\n","\n","1.   There are less Developed countries than in the process. \n","2.   The variation of life expectancy in the countries that are in process of developing  is bigger. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Um9rZFJCWKRV"},"outputs":[],"source":["# Get random data of a country\n","data_Afganistan = df[df['Country'] == 'Afghanistan']\n","\n","# Plot GDP per years\n","plt.plot(data_Afganistan['Year'].astype(int), data_Afganistan['GDP'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qdf-FWnCWXrX"},"outputs":[],"source":["# Plot population per years\n","plt.plot(data_Afganistan['Year'].astype(int), data_Afganistan['Population'])"]},{"cell_type":"markdown","metadata":{"id":"y1QqCZufWa63"},"source":["The real population of Afhanistan in 2015 was 33 millions (source: Wikipedia).\n","This means that data in the following years [2014, 2010,2008,2006, 2005,2003,2001,2000] is wrong, because it has an enormous deviation."]},{"cell_type":"markdown","metadata":{"id":"A1ffQk6GWed_"},"source":["From the above graph we can see that there is a problem in data. The person who entered the data made mistakes in entering the data, sometimes not adding a digit to the number, for example instead of 36043432, he entered only 3604343, and respectively the number is 10 times smaller than the initial one, as a result it creates enormous disturbances in dataset."]},{"cell_type":"markdown","metadata":{"id":"UYsR0FxpWhyO"},"source":["### Plot populatation of multiple countries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6ynX0x2Wnlm"},"outputs":[],"source":["# Plot Population of 26 countries\n","\n","# Get random 26 countries\n","countries_to_analyze = df['Country'].sample(n=26).values\n","\n","# Per each country plot population and outliers.\n","\n","x = y = 0 # coordinates for plot\n","fig, axs = plt.subplots(13, 2, figsize=(12, 40), constrained_layout=True)\n","\n","for country in countries_to_analyze:\n","    # Get all 15 rows (15 years) for selected country\n","    df_country = df[df['Country'] == country]\n","\n","    # Ploting data\n","    axs[x,y].set_title(f'Population of {country}')\n","    axs[x,y].scatter(df_country['Year'].astype(int), df_country['Population'])\n","\n","    # Changing coordinates for ploting on next graph\n","    if y == 0:\n","        y = 1\n","    else:\n","        y = 0\n","        x += 1"]},{"cell_type":"markdown","metadata":{"id":"FXt2y2eFWs4o"},"source":["We can see that the problem lies in most countries.\n","For example, in Greece the population in 2015 was 10.86 million people, but in the graph only 1 million is represented (10 times smaller). Respectively, only one year (2010) has the correct number, the rest have the mistake."]},{"cell_type":"markdown","metadata":{"id":"FovIQG0FWwzS"},"source":["### Plot GDP of multiple countries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AsfrWawxWqEo"},"outputs":[],"source":["# Plot GDP of 26 countries\n","\n","# Get random 26 countries\n","countries_to_analyze = df['Country'].sample(n=26).values\n","\n","# Per each country plot population and outliers.\n","\n","x = y = 0 # coordinates for plot\n","fig, axs = plt.subplots(13, 2, figsize=(12, 40), constrained_layout=True)\n","\n","for country in countries_to_analyze:\n","    # Get all 15 rows (15 years) for selected country\n","    df_country = df[df['Country'] == country]\n","\n","    # Ploting data\n","    axs[x,y].set_title(f'GDP of {country}')\n","    axs[x,y].scatter(df_country['Year'].astype(int), df_country['GDP'])\n","\n","    # Changing coordinates for ploting on next graph\n","    \n","    if y == 0:\n","        y = 1\n","    else:\n","        y = 0\n","        x += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1F-fWPx8W1sL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"TrhKkUVEW_SD"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy4ztqGfXmTv"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykPUCpR4XibD"},"outputs":[],"source":["# Remove white spaces in column names\n","df.columns = df.columns.str.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCLNoxrVXAo4"},"outputs":[],"source":["# drop thinness 5-9\n","df.drop(['thinness 5-9 years'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"MvBzv-8KX_If"},"source":["## Dealing with non-valid data  (GDP and Population)"]},{"cell_type":"markdown","metadata":{"id":"FOkBQCJRZQ4x"},"source":["#### Population "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81dWjeWYYOi1"},"outputs":[],"source":["# Import a new csv file from World Bank Data\n","population_df = pd.read_csv('/content/drive/MyDrive/Organizatii/Sigmoid/Padawan Learning/Exam/World_Bank_Data_Population.csv', error_bad_lines=False, header=2)\n","population_df"]},{"cell_type":"markdown","metadata":{"id":"0GfgNo0oYZMM"},"source":["From the above (new dataset) we can see that it contains total population of each county per years. \n","\n","Now we need to extract data from the new dataset and assign it to the previous one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GwAvA72YWsM"},"outputs":[],"source":["population_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOv3TbdEYdrB"},"outputs":[],"source":["# Drop Non-Usable columns\n","population_droped_df = population_df.drop(['Country Code', 'Indicator Name', 'Indicator Code', 'Unnamed: 66'], axis=1)\n","\n","# Drop Age-Columns that are out of our scope\n","population_droped_df = population_droped_df.drop(\n","    [i for i in population_droped_df.columns[1:] if int(i) < 2000 or int(i) > 2015],\n","    axis=1)\n","\n","population_droped_df"]},{"cell_type":"markdown","metadata":{"id":"6aGgxmwmYiHv"},"source":["Now we need to create a new data set, which will contain the first row - the name of the country and the second row - the year and the third row the population for that year."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDAurFzXYfv-"},"outputs":[],"source":["new_df_population = population_droped_df.melt(id_vars=['Country Name'], value_vars=population_droped_df.columns[1:], var_name='Year', value_name='Population')\n","new_df_population = new_df_population.rename(columns={'Country Name': 'Country'})\n","new_df_population['Year'] = new_df_population['Year'].astype(np.int64)"]},{"cell_type":"markdown","metadata":{"id":"Bj2-q66YYuIe"},"source":["There are two datasets (from WH and WDB). They can contain name of the same country but written differently. Ex: \"USA\" and \"United States of America\" etc.\n","\n","---\n","For handling this problem we will eliminate \"noisy words\" and will search for names that look like most similar between themselves. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdEeXgTVYvJ8"},"outputs":[],"source":["map_countries = {}\n","\n","# Creating bag of words for names of the countries\n","bag_words = ['of', 'republic', 'islam', 'democratic', \"peoples\", 'arab', 'the']\n","\n","def clear_words(name):\n","    # Clear from non-alpha symbols\n","    name = ''.join([i for i in name if i.isalpha() or i == ' '])\n","    return ' '.join(i for i in name.split() if i.lower() not in bag_words)\n","\n","new_df_population['Country'] = new_df_population['Country'].apply(lambda x: clear_words(x))\n","df['Country'] = df['Country'].apply(lambda x: clear_words(x))\n","\n","\n","# Define a function that takes in a country name and returns the closest match\n","def match_name(name, list_of_names, min_score=0):\n","    match = process.extractOne(name, list_of_names, scorer=fuzz.token_sort_ratio)\n","    if match[1] > min_score:\n","        return match[0]\n","    else:\n","        return \"\"\n","\n","# Create a new column in each DataFrame with matched country namesmatch_name\n","for country in df['Country'].unique():\n","    matched_name = match_name(country, new_df_population['Country'].unique())\n","    map_countries[matched_name] = country"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6RSHRX3Y5uM"},"outputs":[],"source":["# Create a new column in each DataFrame with matched country names\n","new_df_population['Matched Country'] = new_df_population['Country'].apply(lambda x: map_countries.get(x))\n","\n","# Merge the DataFrames on the matched country names column\n","merged_df = pd.merge(df, new_df_population, left_on=['Country', 'Year'], right_on=['Matched Country', 'Year'], how='left')\n","\n","# update the population column\n","merged_df['Population'] = merged_df['Population_y']\n","merged_df['Population'] = np.where(merged_df['Population'].isnull(), merged_df['Population_x'], merged_df['Population_y'])\n","\n","merged_df = merged_df.rename(columns = {'Country_x' : 'Country'})\n","df = merged_df.drop(columns=['Population_x', 'Population_y', 'Matched Country', 'Country_y'])"]},{"cell_type":"markdown","metadata":{"id":"9ZR19QxhY71R"},"source":["Ploting new data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_k5vHTSZCgx"},"outputs":[],"source":["# Get random 26 countries\n","countries_to_analyze = merged_df['Country'].sample(n=26).values\n","\n","# Per each country plot population.\n","x = y = 0 # coordinates for plot\n","fig, axs = plt.subplots(13, 2, figsize=(12, 40), constrained_layout=True)\n","\n","for country in countries_to_analyze:\n","    # Get all 15 rows (15 years) for selected country\n","    df_country = merged_df[merged_df['Country'] == country]\n","\n","    # Ploting data\n","    axs[x,y].set_title(f'Population of {country}')\n","    axs[x,y].scatter(df_country['Year'].astype(int), df_country['Population'])\n","\n","    # Changing coordinates for ploting on next graph\n","    if y == 0:\n","        y = 1\n","    else:\n","        y = 0\n","        x += 1"]},{"cell_type":"markdown","metadata":{"id":"YlHVGY6mZJuQ"},"source":["#### GDP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otNu6zhnZX08"},"outputs":[],"source":["gdp_df = pd.read_csv('/content/drive/MyDrive/Organizatii/Sigmoid/Padawan Learning/Exam/World_Bank_Data_GDP.csv', error_bad_lines=False, header=2)\n","gdp_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrdOrjpAZZ6t"},"outputs":[],"source":["# Get columns \n","gdp_df.columns"]},{"cell_type":"markdown","metadata":{"id":"1ypsHpAWZIeC"},"source":["Now we can see that the graph above is smoother"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQMqYSoGZak8"},"outputs":[],"source":["# Drop Non-Usable columns\n","gdp_droped_df = gdp_df.drop(['Country Code', 'Indicator Name', 'Indicator Code', 'Unnamed: 66'], axis=1)\n","\n","# Drop Age-Columns that are out of our scope\n","gdp_droped_df = gdp_droped_df.drop(\n","    [i for i in gdp_droped_df.columns[1:] if int(i) < 2000 or int(i) > 2015],\n","    axis=1)\n","\n","gdp_droped_df"]},{"cell_type":"markdown","metadata":{"id":"I4pMRVWNZc2b"},"source":["Need to create a new dataset, that will contains first row - name of the country, and second row - year, and third row population per that year."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMgXdkMNZCvn"},"outputs":[],"source":["new_df_gdp = gdp_droped_df.melt(id_vars=['Country Name'], value_vars=gdp_droped_df.columns[1:], var_name='Year', value_name='GDP')\n","new_df_gdp = new_df_gdp.rename(columns={'Country Name': 'Country'})\n","new_df_gdp['Year'] = new_df_gdp['Year'].astype(np.int64)"]},{"cell_type":"markdown","metadata":{"id":"eHO4eFudZh_0"},"source":["There are two datasets. They can contain name of the same country but written differently. Ex: \"USA\" and \"United States of America\" etc.\n","\n","---\n","For handling this problem we will eliminate \"noisy words\" and will search for names that look like most similar between themselves. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtIyRtxoZZgg"},"outputs":[],"source":["map_countries = {}\n","\n","# Creating bag of words for names of the countries\n","bag_words = ['of', 'republic', 'islam', 'democratic', \"peoples\", 'arab', 'the']\n","\n","def clear_words(name):\n","    # Clear from non-alpha symbols\n","    name = ''.join([i for i in name if i.isalpha() or i == ' '])\n","    return ' '.join(i for i in name.split() if i.lower() not in bag_words)\n","\n","new_df_gdp['Country'] = new_df_gdp['Country'].apply(lambda x: clear_words(x))\n","df['Country'] = df['Country'].apply(lambda x: clear_words(x))\n","\n","\n","# Define a function that takes in a country name and returns the closest match\n","def match_name(name, list_of_names, min_score=0):\n","    match = process.extractOne(name, list_of_names, scorer=fuzz.token_sort_ratio)\n","    if match[1] > min_score:\n","        return match[0]\n","    else:\n","        return \"\"\n","\n","# Create a new column in each DataFrame with matched country namesmatch_name\n","for country in df['Country'].unique():\n","    matched_name = match_name(country, new_df_gdp['Country'].unique())\n","    map_countries[matched_name] = country"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdU1QMIiZjsW"},"outputs":[],"source":["# Create a new column in each DataFrame with matched country names\n","new_df_gdp['Matched Country'] = new_df_gdp['Country'].apply(lambda x: map_countries.get(x))\n","\n","# Merge the DataFrames on the matched country names column\n","merged_df = pd.merge(df, new_df_gdp, left_on=['Country', 'Year'], right_on=['Matched Country', 'Year'], how='left')\n","\n","# update the GDP column\n","merged_df['GDP'] = merged_df['GDP_y']\n","merged_df['GDP'] = np.where(merged_df['GDP'].isnull(), merged_df['GDP_x'], merged_df['GDP_y'])\n","\n","merged_df = merged_df.rename(columns = {'Country_x' : 'Country'})\n","df = merged_df.drop(columns=['GDP_x', 'GDP_y', 'Matched Country', 'Country_y'])"]},{"cell_type":"markdown","metadata":{"id":"08Dc7Zd9ZlvN"},"source":["Ploting new data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlCNMWBZZo73"},"outputs":[],"source":["# Get random 26 countries\n","countries_to_analyze = merged_df['Country'].sample(n=26).values\n","\n","# Per each country plot GDP.\n","\n","x = y = 0 # coordinates for plot\n","fig, axs = plt.subplots(13, 2, figsize=(12, 40), constrained_layout=True)\n","\n","for country in countries_to_analyze:\n","    # Get all 15 rows (15 years) for selected country\n","    df_country = merged_df[merged_df['Country'] == country]\n","\n","    # Ploting data\n","    axs[x,y].set_title(f'GDP of {country}')\n","    axs[x,y].scatter(df_country['Year'].astype(int), df_country['GDP'])\n","\n","    # Changing coordinates for ploting on next graph\n","    if y == 0:\n","        y = 1\n","    else:\n","        y = 0\n","        x += 1"]},{"cell_type":"markdown","metadata":{"id":"IHZjxY3Sd24y"},"source":["## Dealing with categorical data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcSyRcxsd-ZK"},"outputs":[],"source":["# One hot encoding \"Status\" column\n","df['Status'] = pd.get_dummies(df['Status'].values, drop_first=True).values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRm-lmczehlK"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"G71qdFymaiwN"},"source":["## Spliting in Test and train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DMONKk8arhQ"},"outputs":[],"source":["y = df['Life expectancy']\n","X = df.drop(['Life expectancy'], axis=1)\n","\n","# Split in train/test\n","\n","from sklearn.model_selection import train_test_split\n","training_data, testing_data = train_test_split(df, test_size=0.3, random_state=25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9BKoEqOcMfI"},"outputs":[],"source":["# For better accuracy delete nan values from testing data\n","testing_data = testing_data.dropna()"]},{"cell_type":"markdown","metadata":{"id":"ktCrXap-Z42v"},"source":["## Dealing with NaN values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDmVIFQIZpIi"},"outputs":[],"source":["training_data.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sI7CET9FaAoC"},"outputs":[],"source":["training_data[training_data['Hepatitis B'].isna()]"]},{"cell_type":"markdown","metadata":{"id":"OlFpM_Ojcbt3"},"source":["There is a problem with NaN values at \"Hepatitis B \" columns. Practically 25% of our dataset does not have this data.\n","\n","\n","---\n","\n","From the above cell we can conclude that there is a big variation between minimum and maximum value. That's why we will get mean value based on country Status and Life Expectancy of others countries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KFI-DRNcZmq"},"outputs":[],"source":["# Filling missing data by grouping by country and filling with mean using previous/future values\n","grouped =  training_data.groupby([\"Country\", \"Year\"])\n","\n","# Fill the missing values  with the calculated mean for the same country\n","training_data[\"Alcohol\"] =  training_data.groupby([\"Country\"])[\"Alcohol\"].apply(lambda x: x.fillna(x.mean()))\n","# Fill the missing values in the 'Total expenditure' column with the calculated mean\n","training_data['Total expenditure'] =  training_data.groupby([\"Country\"])['Total expenditure'].apply(lambda x: x.fillna(x.mean()))\n","training_data['GDP'] =  training_data.groupby([\"Country\"])['GDP'].apply(lambda x: x.fillna(x.mean()))\n","training_data['Schooling'] =  training_data.groupby([\"Country\"])['Schooling'].apply(lambda x: x.fillna(x.mean()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTrlrKtGcu-T"},"outputs":[],"source":["training_data['Life expectancy group'] = pd.cut( training_data['Life expectancy'], bins=[0,30,35,40,45,50,55,60, 65, 70, 75, 80, 85, 90, 100], labels=['0-30','30-35','35-40','40-45','45-50','50-55','55-60','60-65', '65-70', '70-75', '75-80', '80-85', '85-90', '90-100'])\n","\n","# Group the dataframe by \"Status\" and \"Life expectancy group\"\n","grouped =  training_data.groupby(['Status', 'Life expectancy group'])\n","\n","# Fill NaN values in columns with mean of the group\n","training_data['Total expenditure'] = grouped['Total expenditure'].transform(lambda x: x.fillna(x.mean()))\n","training_data['Diphtheria'] = grouped['Diphtheria'].transform(lambda x: x.fillna(x.mean()))\n","training_data['thinness  1-19 years'] = grouped['thinness  1-19 years'].transform(lambda x: x.fillna(x.mean()))\n","training_data['Income composition of resources'] = grouped['Income composition of resources'].transform(lambda x: x.fillna(x.mean()))\n","training_data['Schooling'] = grouped['Schooling'].transform(lambda x: x.fillna(x.mean()))\n","training_data['BMI'] = grouped['BMI'].transform(lambda x: x.fillna(x.mean()))\n","training_data['Polio'] = grouped['Polio'].transform(lambda x: x.fillna(x.mean()))\n","training_data['Hepatitis B'] = grouped['Hepatitis B'].transform(lambda x: x.fillna(x.mean()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BrX2jAFc2Vp"},"outputs":[],"source":["training_data[training_data['Hepatitis B'].isna()]"]},{"cell_type":"markdown","metadata":{"id":"w0ft9VsLdinw"},"source":["We can see that majority of Hepatitis B  NaN value dissapeared, but there also remained 8 examples\n","\n","We can see that rows that have missing value in Hepatitics B column have also missing value in others very important columns (Life expectancy, Adult Mortality, etc.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmDTFWc2dgjw"},"outputs":[],"source":["training_data =  training_data.dropna(subset=['Hepatitis B', 'Diphtheria'])\n","training_data[\"Alcohol\"] =  training_data[\"Alcohol\"].fillna(0)\n","training_data.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtxCGMHrh932"},"outputs":[],"source":["# Drop \"Life expectancy group\"\n","training_data.drop(['Life expectancy group'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlSyBq2IjlUN"},"outputs":[],"source":["# Drop Country column\n","training_data = training_data.drop(['Country'], axis=1)\n","testing_data = testing_data.drop(['Country'], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"JZ6zX8q1gn8d"},"source":["## Separating X and y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCoVdAqzgtF3"},"outputs":[],"source":["y_train = training_data['Life expectancy'].values\n","X_train = training_data.drop(['Life expectancy'], axis=1)\n","\n","y_test = testing_data['Life expectancy'].values\n","X_test = testing_data.drop(['Life expectancy'], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"TF6tD_T1hH0c"},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgGtYm5chKkL"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","scaller = MinMaxScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBAxUcWWhTmO"},"outputs":[],"source":["# Fit transform data\n","scaller.fit(X_train)\n","\n","# Transform data\n","X_train = scaller.transform(X_train)\n","X_test= scaller.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"jEDvKs_hjxVw"},"source":["# Training model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBA9RapFjyyY"},"outputs":[],"source":["!pip install lazypredict\n","from lazypredict.Supervised import LazyRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"u1kkyR5Rjzld","outputId":"ea2a0e7d-0f36-4f09-db1a-387671210dac"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 38/42 [03:54<00:43, 10.78s/it]"]},{"name":"stdout","output_type":"stream","text":["[21:48:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 42/42 [03:55<00:00,  5.60s/it]\n"]},{"data":{"text/html":["\n","  <div id=\"df-0011da31-670c-433d-8c09-65f81b039848\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Adjusted R-Squared</th>\n","      <th>R-Squared</th>\n","      <th>RMSE</th>\n","      <th>Time Taken</th>\n","    </tr>\n","    <tr>\n","      <th>Model</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ExtraTreesRegressor</th>\n","      <td>0.96</td>\n","      <td>0.97</td>\n","      <td>1.56</td>\n","      <td>1.01</td>\n","    </tr>\n","    <tr>\n","      <th>LGBMRegressor</th>\n","      <td>0.96</td>\n","      <td>0.96</td>\n","      <td>1.72</td>\n","      <td>0.29</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor</th>\n","      <td>0.96</td>\n","      <td>0.96</td>\n","      <td>1.72</td>\n","      <td>1.99</td>\n","    </tr>\n","    <tr>\n","      <th>BaggingRegressor</th>\n","      <td>0.95</td>\n","      <td>0.95</td>\n","      <td>1.81</td>\n","      <td>0.43</td>\n","    </tr>\n","    <tr>\n","      <th>HistGradientBoostingRegressor</th>\n","      <td>0.95</td>\n","      <td>0.95</td>\n","      <td>1.82</td>\n","      <td>6.97</td>\n","    </tr>\n","    <tr>\n","      <th>XGBRegressor</th>\n","      <td>0.93</td>\n","      <td>0.94</td>\n","      <td>2.12</td>\n","      <td>0.33</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor</th>\n","      <td>0.93</td>\n","      <td>0.94</td>\n","      <td>2.13</td>\n","      <td>0.82</td>\n","    </tr>\n","    <tr>\n","      <th>DecisionTreeRegressor</th>\n","      <td>0.90</td>\n","      <td>0.90</td>\n","      <td>2.59</td>\n","      <td>0.08</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreeRegressor</th>\n","      <td>0.89</td>\n","      <td>0.90</td>\n","      <td>2.70</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor</th>\n","      <td>0.88</td>\n","      <td>0.88</td>\n","      <td>2.85</td>\n","      <td>0.10</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor</th>\n","      <td>0.86</td>\n","      <td>0.87</td>\n","      <td>3.04</td>\n","      <td>0.44</td>\n","    </tr>\n","    <tr>\n","      <th>SVR</th>\n","      <td>0.82</td>\n","      <td>0.82</td>\n","      <td>3.54</td>\n","      <td>0.38</td>\n","    </tr>\n","    <tr>\n","      <th>NuSVR</th>\n","      <td>0.82</td>\n","      <td>0.82</td>\n","      <td>3.54</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>TransformedTargetRegressor</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.72</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>LinearRegression</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.72</td>\n","      <td>0.04</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsIC</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.72</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsCV</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.72</td>\n","      <td>0.09</td>\n","    </tr>\n","    <tr>\n","      <th>RidgeCV</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.72</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>Lars</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.73</td>\n","      <td>0.04</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge</th>\n","      <td>0.80</td>\n","      <td>0.80</td>\n","      <td>3.73</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>PoissonRegressor</th>\n","      <td>0.79</td>\n","      <td>0.80</td>\n","      <td>3.74</td>\n","      <td>0.16</td>\n","    </tr>\n","    <tr>\n","      <th>LassoCV</th>\n","      <td>0.79</td>\n","      <td>0.80</td>\n","      <td>3.74</td>\n","      <td>0.40</td>\n","    </tr>\n","    <tr>\n","      <th>BayesianRidge</th>\n","      <td>0.79</td>\n","      <td>0.80</td>\n","      <td>3.75</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>HuberRegressor</th>\n","      <td>0.79</td>\n","      <td>0.80</td>\n","      <td>3.77</td>\n","      <td>0.15</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNetCV</th>\n","      <td>0.79</td>\n","      <td>0.80</td>\n","      <td>3.78</td>\n","      <td>0.21</td>\n","    </tr>\n","    <tr>\n","      <th>SGDRegressor</th>\n","      <td>0.79</td>\n","      <td>0.79</td>\n","      <td>3.80</td>\n","      <td>0.04</td>\n","    </tr>\n","    <tr>\n","      <th>LarsCV</th>\n","      <td>0.79</td>\n","      <td>0.79</td>\n","      <td>3.81</td>\n","      <td>0.14</td>\n","    </tr>\n","    <tr>\n","      <th>LinearSVR</th>\n","      <td>0.77</td>\n","      <td>0.78</td>\n","      <td>3.95</td>\n","      <td>0.14</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuitCV</th>\n","      <td>0.75</td>\n","      <td>0.76</td>\n","      <td>4.13</td>\n","      <td>0.08</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>0.74</td>\n","      <td>0.75</td>\n","      <td>4.17</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet</th>\n","      <td>0.74</td>\n","      <td>0.75</td>\n","      <td>4.22</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>GammaRegressor</th>\n","      <td>0.73</td>\n","      <td>0.74</td>\n","      <td>4.29</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>TweedieRegressor</th>\n","      <td>0.72</td>\n","      <td>0.73</td>\n","      <td>4.34</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>MLPRegressor</th>\n","      <td>0.61</td>\n","      <td>0.62</td>\n","      <td>5.18</td>\n","      <td>6.98</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuit</th>\n","      <td>0.49</td>\n","      <td>0.51</td>\n","      <td>5.89</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>PassiveAggressiveRegressor</th>\n","      <td>0.48</td>\n","      <td>0.50</td>\n","      <td>5.94</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>RANSACRegressor</th>\n","      <td>0.42</td>\n","      <td>0.44</td>\n","      <td>6.27</td>\n","      <td>0.17</td>\n","    </tr>\n","    <tr>\n","      <th>DummyRegressor</th>\n","      <td>-0.03</td>\n","      <td>-0.00</td>\n","      <td>8.39</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLars</th>\n","      <td>-0.03</td>\n","      <td>-0.00</td>\n","      <td>8.39</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>QuantileRegressor</th>\n","      <td>-0.11</td>\n","      <td>-0.07</td>\n","      <td>8.68</td>\n","      <td>210.35</td>\n","    </tr>\n","    <tr>\n","      <th>GaussianProcessRegressor</th>\n","      <td>-6.23</td>\n","      <td>-6.02</td>\n","      <td>22.19</td>\n","      <td>1.27</td>\n","    </tr>\n","    <tr>\n","      <th>KernelRidge</th>\n","      <td>-69.70</td>\n","      <td>-67.58</td>\n","      <td>69.36</td>\n","      <td>0.65</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0011da31-670c-433d-8c09-65f81b039848')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0011da31-670c-433d-8c09-65f81b039848 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0011da31-670c-433d-8c09-65f81b039848');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n","Model                                                                         \n","ExtraTreesRegressor                          0.96       0.97  1.56        1.01\n","LGBMRegressor                                0.96       0.96  1.72        0.29\n","RandomForestRegressor                        0.96       0.96  1.72        1.99\n","BaggingRegressor                             0.95       0.95  1.81        0.43\n","HistGradientBoostingRegressor                0.95       0.95  1.82        6.97\n","XGBRegressor                                 0.93       0.94  2.12        0.33\n","GradientBoostingRegressor                    0.93       0.94  2.13        0.82\n","DecisionTreeRegressor                        0.90       0.90  2.59        0.08\n","ExtraTreeRegressor                           0.89       0.90  2.70        0.03\n","KNeighborsRegressor                          0.88       0.88  2.85        0.10\n","AdaBoostRegressor                            0.86       0.87  3.04        0.44\n","SVR                                          0.82       0.82  3.54        0.38\n","NuSVR                                        0.82       0.82  3.54        0.62\n","TransformedTargetRegressor                   0.80       0.80  3.72        0.02\n","LinearRegression                             0.80       0.80  3.72        0.04\n","LassoLarsIC                                  0.80       0.80  3.72        0.06\n","LassoLarsCV                                  0.80       0.80  3.72        0.09\n","RidgeCV                                      0.80       0.80  3.72        0.03\n","Lars                                         0.80       0.80  3.73        0.04\n","Ridge                                        0.80       0.80  3.73        0.01\n","PoissonRegressor                             0.79       0.80  3.74        0.16\n","LassoCV                                      0.79       0.80  3.74        0.40\n","BayesianRidge                                0.79       0.80  3.75        0.06\n","HuberRegressor                               0.79       0.80  3.77        0.15\n","ElasticNetCV                                 0.79       0.80  3.78        0.21\n","SGDRegressor                                 0.79       0.79  3.80        0.04\n","LarsCV                                       0.79       0.79  3.81        0.14\n","LinearSVR                                    0.77       0.78  3.95        0.14\n","OrthogonalMatchingPursuitCV                  0.75       0.76  4.13        0.08\n","Lasso                                        0.74       0.75  4.17        0.06\n","ElasticNet                                   0.74       0.75  4.22        0.03\n","GammaRegressor                               0.73       0.74  4.29        0.02\n","TweedieRegressor                             0.72       0.73  4.34        0.02\n","MLPRegressor                                 0.61       0.62  5.18        6.98\n","OrthogonalMatchingPursuit                    0.49       0.51  5.89        0.05\n","PassiveAggressiveRegressor                   0.48       0.50  5.94        0.06\n","RANSACRegressor                              0.42       0.44  6.27        0.17\n","DummyRegressor                              -0.03      -0.00  8.39        0.03\n","LassoLars                                   -0.03      -0.00  8.39        0.05\n","QuantileRegressor                           -0.11      -0.07  8.68      210.35\n","GaussianProcessRegressor                    -6.23      -6.02 22.19        1.27\n","KernelRidge                                -69.70     -67.58 69.36        0.65"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["reg = LazyRegressor(predictions=True)\n","models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n","models"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNS8d3W7rIp2JPOlBLtY9j8"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
